[{"id":"7313417715","type":"WatchEvent","actor":{"id":306951,"login":"nadesai","display_login":"nadesai","gravatar_id":"","url":"https://api.github.com/users/nadesai","avatar_url":"https://avatars.githubusercontent.com/u/306951?"},"repo":{"id":56853393,"name":"polytypic/fastener","url":"https://api.github.com/repos/polytypic/fastener"},"payload":{"action":"started"},"public":true,"created_at":"2018-03-01T01:00:57Z"},{"id":"7308879577","type":"WatchEvent","actor":{"id":306951,"login":"nadesai","display_login":"nadesai","gravatar_id":"","url":"https://api.github.com/users/nadesai","avatar_url":"https://avatars.githubusercontent.com/u/306951?"},"repo":{"id":51808714,"name":"TheTorProject/gettorbrowser","url":"https://api.github.com/repos/TheTorProject/gettorbrowser"},"payload":{"action":"started"},"public":true,"created_at":"2018-02-28T09:53:05Z","org":{"id":3301912,"login":"TheTorProject","gravatar_id":"","url":"https://api.github.com/orgs/TheTorProject","avatar_url":"https://avatars.githubusercontent.com/u/3301912?"}},{"id":"7299364149","type":"WatchEvent","actor":{"id":306951,"login":"nadesai","display_login":"nadesai","gravatar_id":"","url":"https://api.github.com/users/nadesai","avatar_url":"https://avatars.githubusercontent.com/u/306951?"},"repo":{"id":66940339,"name":"GlobalFishingWatch/vessel-classification","url":"https://api.github.com/repos/GlobalFishingWatch/vessel-classification"},"payload":{"action":"started"},"public":true,"created_at":"2018-02-26T19:52:53Z","org":{"id":9467707,"login":"GlobalFishingWatch","gravatar_id":"","url":"https://api.github.com/orgs/GlobalFishingWatch","avatar_url":"https://avatars.githubusercontent.com/u/9467707?"}},{"id":"7294642860","type":"ForkEvent","actor":{"id":306951,"login":"nadesai","display_login":"nadesai","gravatar_id":"","url":"https://api.github.com/users/nadesai","avatar_url":"https://avatars.githubusercontent.com/u/306951?"},"repo":{"id":47419579,"name":"andreasgrill/auto-selfcontrol","url":"https://api.github.com/repos/andreasgrill/auto-selfcontrol"},"payload":{"forkee":{"id":122896160,"name":"auto-selfcontrol","full_name":"nadesai/auto-selfcontrol","owner":{"login":"nadesai","id":306951,"avatar_url":"https://avatars3.githubusercontent.com/u/306951?v=4","gravatar_id":"","url":"https://api.github.com/users/nadesai","html_url":"https://github.com/nadesai","followers_url":"https://api.github.com/users/nadesai/followers","following_url":"https://api.github.com/users/nadesai/following{/other_user}","gists_url":"https://api.github.com/users/nadesai/gists{/gist_id}","starred_url":"https://api.github.com/users/nadesai/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nadesai/subscriptions","organizations_url":"https://api.github.com/users/nadesai/orgs","repos_url":"https://api.github.com/users/nadesai/repos","events_url":"https://api.github.com/users/nadesai/events{/privacy}","received_events_url":"https://api.github.com/users/nadesai/received_events","type":"User","site_admin":false},"private":false,"html_url":"https://github.com/nadesai/auto-selfcontrol","description":"Small utility to schedule start and stop times of SelfControl","fork":true,"url":"https://api.github.com/repos/nadesai/auto-selfcontrol","forks_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/forks","keys_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/keys{/key_id}","collaborators_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/collaborators{/collaborator}","teams_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/teams","hooks_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/hooks","issue_events_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/issues/events{/number}","events_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/events","assignees_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/assignees{/user}","branches_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/branches{/branch}","tags_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/tags","blobs_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/git/blobs{/sha}","git_tags_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/git/tags{/sha}","git_refs_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/git/refs{/sha}","trees_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/git/trees{/sha}","statuses_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/statuses/{sha}","languages_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/languages","stargazers_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/stargazers","contributors_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/contributors","subscribers_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/subscribers","subscription_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/subscription","commits_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/commits{/sha}","git_commits_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/git/commits{/sha}","comments_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/comments{/number}","issue_comment_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/issues/comments{/number}","contents_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/contents/{+path}","compare_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/compare/{base}...{head}","merges_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/merges","archive_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/{archive_format}{/ref}","downloads_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/downloads","issues_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/issues{/number}","pulls_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/pulls{/number}","milestones_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/milestones{/number}","notifications_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/notifications{?since,all,participating}","labels_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/labels{/name}","releases_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/releases{/id}","deployments_url":"https://api.github.com/repos/nadesai/auto-selfcontrol/deployments","created_at":"2018-02-26T01:32:16Z","updated_at":"2018-02-02T22:13:40Z","pushed_at":"2018-02-09T21:57:54Z","git_url":"git://github.com/nadesai/auto-selfcontrol.git","ssh_url":"git@github.com:nadesai/auto-selfcontrol.git","clone_url":"https://github.com/nadesai/auto-selfcontrol.git","svn_url":"https://github.com/nadesai/auto-selfcontrol","homepage":null,"size":28,"stargazers_count":0,"watchers_count":0,"language":null,"has_issues":false,"has_projects":true,"has_downloads":true,"has_wiki":true,"has_pages":false,"forks_count":0,"mirror_url":null,"archived":false,"open_issues_count":0,"license":{"key":"mit","name":"MIT License","spdx_id":"MIT","url":"https://api.github.com/licenses/mit"},"forks":0,"open_issues":0,"watchers":0,"default_branch":"master","public":true}},"public":true,"created_at":"2018-02-26T01:32:16Z"},{"id":"7290608314","type":"WatchEvent","actor":{"id":306951,"login":"nadesai","display_login":"nadesai","gravatar_id":"","url":"https://api.github.com/users/nadesai","avatar_url":"https://avatars.githubusercontent.com/u/306951?"},"repo":{"id":1918497,"name":"apache/derby","url":"https://api.github.com/repos/apache/derby"},"payload":{"action":"started"},"public":true,"created_at":"2018-02-24T03:46:06Z","org":{"id":47359,"login":"apache","gravatar_id":"","url":"https://api.github.com/orgs/apache","avatar_url":"https://avatars.githubusercontent.com/u/47359?"}},{"id":"7289921763","type":"WatchEvent","actor":{"id":306951,"login":"nadesai","display_login":"nadesai","gravatar_id":"","url":"https://api.github.com/users/nadesai","avatar_url":"https://avatars.githubusercontent.com/u/306951?"},"repo":{"id":117000645,"name":"developmentseed/label-maker","url":"https://api.github.com/repos/developmentseed/label-maker"},"payload":{"action":"started"},"public":true,"created_at":"2018-02-23T22:34:59Z","org":{"id":92384,"login":"developmentseed","gravatar_id":"","url":"https://api.github.com/orgs/developmentseed","avatar_url":"https://avatars.githubusercontent.com/u/92384?"}},{"id":"7278626866","type":"WatchEvent","actor":{"id":306951,"login":"nadesai","display_login":"nadesai","gravatar_id":"","url":"https://api.github.com/users/nadesai","avatar_url":"https://avatars.githubusercontent.com/u/306951?"},"repo":{"id":60374819,"name":"jupyterlab/jupyterlab","url":"https://api.github.com/repos/jupyterlab/jupyterlab"},"payload":{"action":"started"},"public":true,"created_at":"2018-02-21T23:47:14Z","org":{"id":22800682,"login":"jupyterlab","gravatar_id":"","url":"https://api.github.com/orgs/jupyterlab","avatar_url":"https://avatars.githubusercontent.com/u/22800682?"}},{"id":"7278606436","type":"WatchEvent","actor":{"id":306951,"login":"nadesai","display_login":"nadesai","gravatar_id":"","url":"https://api.github.com/users/nadesai","avatar_url":"https://avatars.githubusercontent.com/u/306951?"},"repo":{"id":11646008,"name":"usgs/libcomcat","url":"https://api.github.com/repos/usgs/libcomcat"},"payload":{"action":"started"},"public":true,"created_at":"2018-02-21T23:41:04Z","org":{"id":1091434,"login":"usgs","gravatar_id":"","url":"https://api.github.com/orgs/usgs","avatar_url":"https://avatars.githubusercontent.com/u/1091434?"}},{"id":"7267103039","type":"PushEvent","actor":{"id":306951,"login":"nadesai","display_login":"nadesai","gravatar_id":"","url":"https://api.github.com/users/nadesai","avatar_url":"https://avatars.githubusercontent.com/u/306951?"},"repo":{"id":17346437,"name":"nadesai/spark","url":"https://api.github.com/repos/nadesai/spark"},"payload":{"push_id":2342904672,"size":1000,"distinct_size":1400,"ref":"refs/heads/master","head":"3ee3b2ae1ff8fbeb43a08becef43a9bd763b06bb","before":"5c8edfc4a8864f4091998901bbca062cd8466b6f","commits":[{"sha":"2f962422a25020582c915e15819f91f43c0b9d68","author":{"email":"ybliang8@gmail.com","name":"Yanbo Liang"},"message":"[MINOR][ML] Remove unnecessary default value setting for evaluators.\n\n## What changes were proposed in this pull request?\nRemove unnecessary default value setting for all evaluators, as we have set them in corresponding _HasXXX_ base classes.\n\n## How was this patch tested?\nExisting tests.\n\nAuthor: Yanbo Liang <ybliang8@gmail.com>\n\nCloses #19262 from yanboliang/evaluation.","distinct":true,"url":"https://api.github.com/repos/nadesai/spark/commits/2f962422a25020582c915e15819f91f43c0b9d68"},{"sha":"d5aefa83ad8608fbea7c08e8d9164f8bee00863d","author":{"email":"huaxing@us.ibm.com","name":"Huaxin Gao"},"message":"[SPARK-21338][SQL] implement isCascadingTruncateTable() method in AggregatedDialect\n\n## What changes were proposed in this pull request?\n\norg.apache.spark.sql.jdbc.JdbcDialect's method:\ndef isCascadingTruncateTable(): Option[Boolean] = None\nis not overriden in org.apache.spark.sql.jdbc.AggregatedDialect class.\nBecause of this issue, when you add more than one dialect Spark doesn't truncate table because isCascadingTruncateTable always returns default None for Aggregated Dialect.\nWill implement isCascadingTruncateTable in AggregatedDialect class in this PR.\n\n## How was this patch tested?\n\nIn JDBCSuite, inside test(\"Aggregated dialects\"), will add one line to test AggregatedDialect.isCascadingTruncateTable\n\nAuthor: Huaxin Gao <huaxing@us.ibm.com>\n\nCloses #19256 from huaxingao/spark-21338.","distinct":true,"url":"https://api.github.com/repos/nadesai/spark/commits/d5aefa83ad8608fbea7c08e8d9164f8bee00863d"},{"sha":"ee13f3e3dc3faa5152cefa91c22f8aaa8e425bb4","author":{"email":"anton.okolnychyi@sap.com","name":"aokolnychyi"},"message":"[SPARK-21969][SQL] CommandUtils.updateTableStats should call refreshTable\n\n## What changes were proposed in this pull request?\n\nTables in the catalog cache are not invalidated once their statistics are updated. As a consequence, existing sessions will use the cached information even though it is not valid anymore. Consider and an example below.\n\n```\n// step 1\nspark.range(100).write.saveAsTable(\"tab1\")\n// step 2\nspark.sql(\"analyze table tab1 compute statistics\")\n// step 3\nspark.sql(\"explain cost select distinct * from tab1\").show(false)\n// step 4\nspark.range(100).write.mode(\"append\").saveAsTable(\"tab1\")\n// step 5\nspark.sql(\"explain cost select distinct * from tab1\").show(false)\n```\n\nAfter step 3, the table will be present in the catalog relation cache. Step 4 will correctly update the metadata inside the catalog but will NOT invalidate the cache.\n\nBy the way, ``spark.sql(\"analyze table tab1 compute statistics\")`` between step 3 and step 4 would also solve the problem.\n\n## How was this patch tested?\n\nCurrent and additional unit tests.\n\nAuthor: aokolnychyi <anton.okolnychyi@sap.com>\n\nCloses #19252 from aokolnychyi/spark-21969.","distinct":true,"url":"https://api.github.com/repos/nadesai/spark/commits/ee13f3e3dc3faa5152cefa91c22f8aaa8e425bb4"},{"sha":"718bbc939037929ef5b8f4b4fe10aadfbab4408e","author":{"email":"cutlerb@gmail.com","name":"Bryan Cutler"},"message":"[SPARK-22067][SQL] ArrowWriter should use position when setting UTF8String ByteBuffer\n\n## What changes were proposed in this pull request?\n\nThe ArrowWriter StringWriter was setting Arrow data using a position of 0 instead of the actual position in the ByteBuffer.  This was currently working because of a bug ARROW-1443, and has been fixed as of\nArrow 0.7.0.  Testing with this version revealed the error in ArrowConvertersSuite test string conversion.\n\n## How was this patch tested?\n\nExisting tests, manually verified working with Arrow 0.7.0\n\nAuthor: Bryan Cutler <cutlerb@gmail.com>\n\nCloses #19284 from BryanCutler/arrow-ArrowWriter-StringWriter-position-SPARK-22067.","distinct":true,"url":"https://api.github.com/repos/nadesai/spark/commits/718bbc939037929ef5b8f4b4fe10aadfbab4408e"},{"sha":"c6ff59a230758b409fa9cc548b7d283eeb7ebe5d","author":{"email":"vanzin@cloudera.com","name":"Marcelo Vanzin"},"message":"[SPARK-18838][CORE] Add separate listener queues to LiveListenerBus.\n\nThis change modifies the live listener bus so that all listeners are\nadded to queues; each queue has its own thread to dispatch events,\nmaking it possible to separate slow listeners from other more\nperformance-sensitive ones.\n\nThe public API has not changed - all listeners added with the existing\n\"addListener\" method, which after this change mostly means all\nuser-defined listeners, end up in a default queue. Internally, there's\nan API allowing listeners to be added to specific queues, and that API\nis used to separate the internal Spark listeners into 3 categories:\napplication status listeners (e.g. UI), executor management (e.g. dynamic\nallocation), and the event log.\n\nThe queueing logic, while abstracted away in a separate class, is kept\nas much as possible hidden away from consumers. Aside from choosing their\nqueue, there's no code change needed to take advantage of queues.\n\nTest coverage relies on existing tests; a few tests had to be tweaked\nbecause they relied on `LiveListenerBus.postToAll` being synchronous,\nand the change makes that method asynchronous. Other tests were simplified\nnot to use the asynchronous LiveListenerBus.\n\nAuthor: Marcelo Vanzin <vanzin@cloudera.com>\n\nCloses #19211 from vanzin/SPARK-18838.","distinct":true,"url":"https://api.github.com/repos/nadesai/spark/commits/c6ff59a230758b409fa9cc548b7d283eeb7ebe5d"},{"sha":"280ff523f4079dd9541efc95e6efcb69f9374d22","author":{"email":"brkyvz@gmail.com","name":"Burak Yavuz"},"message":"[SPARK-21977] SinglePartition optimizations break certain Streaming Stateful Aggregation requirements\n\n## What changes were proposed in this pull request?\n\nThis is a bit hard to explain as there are several issues here, I'll try my best. Here are the requirements:\n  1. A StructuredStreaming Source that can generate empty RDDs with 0 partitions\n  2. A StructuredStreaming query that uses the above source, performs a stateful aggregation\n     (mapGroupsWithState, groupBy.count, ...), and coalesce's by 1\n\nThe crux of the problem is that when a dataset has a `coalesce(1)` call, it receives a `SinglePartition` partitioning scheme. This scheme satisfies most required distributions used for aggregations such as HashAggregateExec. This causes a world of problems:\n  Symptom 1. If the input RDD has 0 partitions, the whole lineage will receive 0 partitions, nothing will be executed, the state store will not create any delta files. When this happens, the next trigger fails, because the StateStore fails to load the delta file for the previous trigger\n  Symptom 2. Let's say that there was data. Then in this case, if you stop your stream, and change `coalesce(1)` with `coalesce(2)`, then restart your stream, your stream will fail, because `spark.sql.shuffle.partitions - 1` number of StateStores will fail to find its delta files.\n\nTo fix the issues above, we must check that the partitioning of the child of a `StatefulOperator` satisfies:\nIf the grouping expressions are empty:\n  a) AllTuple distribution\n  b) Single physical partition\nIf the grouping expressions are non empty:\n  a) Clustered distribution\n  b) spark.sql.shuffle.partition # of partitions\nwhether or not `coalesce(1)` exists in the plan, and whether or not the input RDD for the trigger has any data.\n\nOnce you fix the above problem by adding an Exchange to the plan, you come across the following bug:\nIf you call `coalesce(1).groupBy().count()` on a Streaming DataFrame, and if you have a trigger with no data, `StateStoreRestoreExec` doesn't return the prior state. However, for this specific aggregation, `HashAggregateExec` after the restore returns a (0, 0) row, since we're performing a count, and there is no data. Then this data gets stored in `StateStoreSaveExec` causing the previous counts to be overwritten and lost.\n\n## How was this patch tested?\n\nRegression tests\n\nAuthor: Burak Yavuz <brkyvz@gmail.com>\n\nCloses #19196 from brkyvz/sa-0.","distinct":true,"url":"https://api.github.com/repos/nadesai/spark/commits/280ff523f4079dd9541efc95e6efcb69f9374d22"},{"sha":"3d4dd14cd563cef2aee791dc7c15d247c076f9a1","author":{"email":"sowen@cloudera.com","name":"Sean Owen"},"message":"[SPARK-22066][BUILD] Update checkstyle to 8.2, enable it, fix violations\n\n## What changes were proposed in this pull request?\n\nUpdate plugins, including scala-maven-plugin, to latest versions. Update checkstyle to 8.2. Remove bogus checkstyle config and enable it. Fix existing and new Java checkstyle errors.\n\n## How was this patch tested?\n\nExisting tests\n\nAuthor: Sean Owen <sowen@cloudera.com>\n\nCloses #19282 from srowen/SPARK-22066.","distinct":true,"url":"https://api.github.com/repos/nadesai/spark/commits/3d4dd14cd563cef2aee791dc7c15d247c076f9a1"},{"sha":"2b6ff0cefda2a44fb265b4ac08b95ac4101d9f21","author":{"email":"sowen@cloudera.com","name":"Sean Owen"},"message":"[SPARK-22066][BUILD][HOTFIX] Revert scala-maven-plugin to 3.2.2 to work with Maven+zinc again\n\n## What changes were proposed in this pull request?\n\nSee https://github.com/apache/spark/pull/19282\nRevert scala-maven-plugin to 3.2.2 to work with Maven+zinc again\n\n## How was this patch tested?\n\nReproduced locally with zinc, and confirmed this removes the problem.\n\nAuthor: Sean Owen <sowen@cloudera.com>\n\nCloses #19292 from srowen/SPARK-22066.2.","distinct":true,"url":"https://api.github.com/repos/nadesai/spark/commits/2b6ff0cefda2a44fb265b4ac08b95ac4101d9f21"},{"sha":"e17901d6df42edf2c7a3460995a0e954ad9a159f","author":{"email":"sowen@cloudera.com","name":"Sean Owen"},"message":"[SPARK-22049][DOCS] Confusing behavior of from_utc_timestamp and to_utc_timestamp\n\n## What changes were proposed in this pull request?\n\nClarify behavior of to_utc_timestamp/from_utc_timestamp with an example\n\n## How was this patch tested?\n\nDoc only change / existing tests\n\nAuthor: Sean Owen <sowen@cloudera.com>\n\nCloses #19276 from srowen/SPARK-22049.","distinct":true,"url":"https://api.github.com/repos/nadesai/spark/commits/e17901d6df42edf2c7a3460995a0e954ad9a159f"},{"sha":"ce6a71e013c403d0a3690cf823934530ce0ea5ef","author":{"email":"wenchen@databricks.com","name":"Wenchen Fan"},"message":"[SPARK-22076][SQL] Expand.projections should not be a Stream\n\n## What changes were proposed in this pull request?\n\nSpark with Scala 2.10 fails with a group by cube:\n```\nspark.range(1).select($\"id\" as \"a\", $\"id\" as \"b\").write.partitionBy(\"a\").mode(\"overwrite\").saveAsTable(\"rollup_bug\")\nspark.sql(\"select 1 from rollup_bug group by rollup ()\").show\n```\n\nIt can be traced back to https://github.com/apache/spark/pull/15484 , which made `Expand.projections` a lazy `Stream` for group by cube.\n\nIn scala 2.10 `Stream` captures a lot of stuff, and in this case it captures the entire query plan which has some un-serializable parts.\n\nThis change is also good for master branch, to reduce the serialized size of `Expand.projections`.\n\n## How was this patch tested?\n\nmanually verified with Spark with Scala 2.10.\n\nAuthor: Wenchen Fan <wenchen@databricks.com>\n\nCloses #19289 from cloud-fan/bug.","distinct":true,"url":"https://api.github.com/repos/nadesai/spark/commits/ce6a71e013c403d0a3690cf823934530ce0ea5ef"},{"sha":"bb9c0697d509880d30c874d7f6384d7633aed061","author":{"email":"vanzin@cloudera.com","name":"Marcelo Vanzin"},"message":"[SPARK-18838][HOTFIX][YARN] Check internal context state before stopping it.\n\nThe live listener bus now cleans up after itself and releases listeners\nafter stopping, so code cannot get references to listeners after the\nSpark context is stopped.\n\nAuthor: Marcelo Vanzin <vanzin@cloudera.com>\n\nCloses #19297 from vanzin/SPARK-18838.hotfix.","distinct":true,"url":"https://api.github.com/repos/nadesai/spark/commits/bb9c0697d509880d30c874d7f6384d7633aed061"},{"sha":"55d5fa79db883e4d93a9c102a94713c9d2d1fb55","author":{"email":"devaraj@apache.org","name":"Devaraj K"},"message":"[SPARK-21384][YARN] Spark + YARN fails with LocalFileSystem as default FS\n\n## What changes were proposed in this pull request?\n\nWhen the libraries temp directory(i.e. __spark_libs__*.zip dir) file system and staging dir(destination) file systems are the same then the __spark_libs__*.zip is not copying to the staging directory. But after making this decision the libraries zip file is getting deleted immediately and becoming unavailable for the Node Manager's localization.\n\nWith this change, client copies the files to remote always when the source scheme is \"file\".\n\n## How was this patch tested?\n\nI have verified it manually in yarn/cluster and yarn/client modes with hdfs and local file systems.\n\nAuthor: Devaraj K <devaraj@apache.org>\n\nCloses #19141 from devaraj-kavali/SPARK-21384.","distinct":true,"url":"https://api.github.com/repos/nadesai/spark/commits/55d5fa79db883e4d93a9c102a94713c9d2d1fb55"},{"sha":"352bea5457bf77bb6d0e45a8c71be104bee8a586","author":{"email":"wenchen@databricks.com","name":"Wenchen Fan"},"message":"[SPARK-22076][SQL][FOLLOWUP] Expand.projections should not be a Stream\n\n## What changes were proposed in this pull request?\n\nThis a follow-up of https://github.com/apache/spark/pull/19289 , we missed another place: `rollup`. `Seq.init.toSeq` also returns a `Stream`, we should fix it too.\n\n## How was this patch tested?\n\nmanually\n\nAuthor: Wenchen Fan <wenchen@databricks.com>\n\nCloses #19298 from cloud-fan/bug.","distinct":true,"url":"https://api.github.com/repos/nadesai/spark/commits/352bea5457bf77bb6d0e45a8c71be104bee8a586"},{"sha":"1da5822e6a60e024ae31cd722ca04abb2e7aed43","author":{"email":"sshao@hortonworks.com","name":"jerryshao"},"message":"[SPARK-21934][CORE] Expose Shuffle Netty memory usage to MetricsSystem\n\n## What changes were proposed in this pull request?\n\nThis is a followup work of SPARK-9104 to expose the Netty memory usage to MetricsSystem. Current the shuffle Netty memory usage of `NettyBlockTransferService` will be exposed, if using external shuffle, then the Netty memory usage of `ExternalShuffleClient` and `ExternalShuffleService` will be exposed instead. Currently I don't expose Netty memory usage of `YarnShuffleService`, because `YarnShuffleService` doesn't have `MetricsSystem` itself, and is better to connect to Hadoop's MetricsSystem.\n\n## How was this patch tested?\n\nManually verified in local cluster.\n\nAuthor: jerryshao <sshao@hortonworks.com>\n\nCloses #19160 from jerryshao/SPARK-21934.","distinct":true,"url":"https://api.github.com/repos/nadesai/spark/commits/1da5822e6a60e024ae31cd722ca04abb2e7aed43"},{"sha":"a8d9ec8a60f21abb520b9109b238f914d2449022","author":{"email":"gurwls223@gmail.com","name":"hyukjinkwon"},"message":"[SPARK-21780][R] Simpler Dataset.sample API in R\n\n## What changes were proposed in this pull request?\n\nThis PR make `sample(...)` able to omit `withReplacement` defaulting to `FALSE`.\n\nIn short, the following examples are allowed:\n\n```r\n> df <- createDataFrame(as.list(seq(10)))\n> count(sample(df, fraction=0.5, seed=3))\n[1] 4\n> count(sample(df, fraction=1.0))\n[1] 10\n```\n\nIn addition, this PR also adds some type checking logics as below:\n\n```r\n> sample(df, fraction = \"a\")\nError in sample(df, fraction = \"a\") :\n  fraction must be numeric; however, got character\n> sample(df, fraction = 1, seed = NULL)\nError in sample(df, fraction = 1, seed = NULL) :\n  seed must not be NULL or NA; however, got NULL\n> sample(df, list(1), 1.0)\nError in sample(df, list(1), 1) :\n  withReplacement must be logical; however, got list\n> sample(df, fraction = -1.0)\n...\nError in sample : illegal argument - requirement failed: Sampling fraction (-1.0) must be on interval [0, 1] without replacement\n```\n\n## How was this patch tested?\n\nManually tested, unit tests added in `R/pkg/tests/fulltests/test_sparkSQL.R`.\n\nAuthor: hyukjinkwon <gurwls223@gmail.com>\n\nCloses #19243 from HyukjinKwon/SPARK-21780.","distinct":true,"url":"https://api.github.com/repos/nadesai/spark/commits/a8d9ec8a60f21abb520b9109b238f914d2449022"},{"sha":"1d1a09be9fe07244d7d12442f6105f823b260755","author":{"email":"wangzhenhua@huawei.com","name":"Zhenhua Wang"},"message":"[SPARK-17997][SQL] Add an aggregation function for counting distinct values for multiple intervals\n\n## What changes were proposed in this pull request?\n\nThis work is a part of [SPARK-17074](https://issues.apache.org/jira/browse/SPARK-17074) to compute equi-height histograms. Equi-height histogram is an array of bins. A bin consists of two endpoints which form an interval of values and the ndv in that interval.\n\nThis PR creates a new aggregate function, given an array of endpoints, counting distinct values (ndv) in intervals among those endpoints.\n\nThis PR also refactors `HyperLogLogPlusPlus` by extracting a helper class `HyperLogLogPlusPlusHelper`, where the underlying HLLPP algorithm locates.\n\n## How was this patch tested?\n\nAdd new test cases.\n\nAuthor: Zhenhua Wang <wangzhenhua@huawei.com>\n\nCloses #15544 from wzhfy/countIntervals.","distinct":true,"url":"https://api.github.com/repos/nadesai/spark/commits/1d1a09be9fe07244d7d12442f6105f823b260755"},{"sha":"1270e71753f40c353fb726a0a3d373d181aedb40","author":{"email":"viirya@gmail.com","name":"Liang-Chi Hsieh"},"message":"[SPARK-22086][DOCS] Add expression description for CASE WHEN\n\n## What changes were proposed in this pull request?\n\nIn SQL conditional expressions, only CASE WHEN lacks for expression description. This patch fills the gap.\n\n## How was this patch tested?\n\nOnly documentation change.\n\nAuthor: Liang-Chi Hsieh <viirya@gmail.com>\n\nCloses #19304 from viirya/casewhen-doc.","distinct":true,"url":"https://api.github.com/repos/nadesai/spark/commits/1270e71753f40c353fb726a0a3d373d181aedb40"},{"sha":"f10cbf17dc7ceb96982fcdc964c849336fb50deb","author":{"email":"sowen@cloudera.com","name":"Sean Owen"},"message":"[SPARK-21977][HOTFIX] Adjust EnsureStatefulOpPartitioningSuite to use scalatest lifecycle normally instead of constructor\n\n## What changes were proposed in this pull request?\n\nAdjust EnsureStatefulOpPartitioningSuite to use scalatest lifecycle normally instead of constructor; fixes:\n\n```\n*** RUN ABORTED ***\n  org.apache.spark.SparkException: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:\norg.apache.spark.sql.streaming.EnsureStatefulOpPartitioningSuite.<init>(EnsureStatefulOpPartitioningSuite.scala:35)\n```\n\n## How was this patch tested?\n\nExisting tests\n\nAuthor: Sean Owen <sowen@cloudera.com>\n\nCloses #19306 from srowen/SPARK-21977.2.","distinct":true,"url":"https://api.github.com/repos/nadesai/spark/commits/f10cbf17dc7ceb96982fcdc964c849336fb50deb"},{"sha":"b75bd1777496ce0354458bf85603a8087a6a0ff8","author":{"email":"irashid@cloudera.com","name":"Imran Rashid"},"message":"[SPARK-21928][CORE] Set classloader on SerializerManager's private kryo\n\n## What changes were proposed in this pull request?\n\nWe have to make sure that SerializerManager's private instance of\nkryo also uses the right classloader, regardless of the current thread\nclassloader.  In particular, this fixes serde during remote cache\nfetches, as those occur in netty threads.\n\n## How was this patch tested?\n\nManual tests & existing suite via jenkins.  I haven't been able to reproduce this is in a unit test, because when a remote RDD partition can be fetched, there is a warning message and then the partition is just recomputed locally.  I manually verified the warning message is no longer present.\n\nAuthor: Imran Rashid <irashid@cloudera.com>\n\nCloses #19280 from squito/SPARK-21928_ser_classloader.","distinct":true,"url":"https://api.github.com/repos/nadesai/spark/commits/b75bd1777496ce0354458bf85603a8087a6a0ff8"},{"sha":"f7ad0dbd512b00241337f15e1ed0a28a1d30711d","author":{"email":"vanzin@cloudera.com","name":"Marcelo Vanzin"},"message":"[INFRA] Close stale PRs.\n\nCloses #19296\nCloses #19291","distinct":true,"url":"https://api.github.com/repos/nadesai/spark/commits/f7ad0dbd512b00241337f15e1ed0a28a1d30711d"}]},"public":true,"created_at":"2018-02-20T02:30:13Z"},{"id":"7266653137","type":"WatchEvent","actor":{"id":306951,"login":"nadesai","display_login":"nadesai","gravatar_id":"","url":"https://api.github.com/users/nadesai","avatar_url":"https://avatars.githubusercontent.com/u/306951?"},"repo":{"id":55077261,"name":"rust-lang-nursery/futures-rs","url":"https://api.github.com/repos/rust-lang-nursery/futures-rs"},"payload":{"action":"started"},"public":true,"created_at":"2018-02-19T23:24:56Z","org":{"id":14631425,"login":"rust-lang-nursery","gravatar_id":"","url":"https://api.github.com/orgs/rust-lang-nursery","avatar_url":"https://avatars.githubusercontent.com/u/14631425?"}},{"id":"7259609777","type":"WatchEvent","actor":{"id":306951,"login":"nadesai","display_login":"nadesai","gravatar_id":"","url":"https://api.github.com/users/nadesai","avatar_url":"https://avatars.githubusercontent.com/u/306951?"},"repo":{"id":72343220,"name":"QuantStack/xtensor","url":"https://api.github.com/repos/QuantStack/xtensor"},"payload":{"action":"started"},"public":true,"created_at":"2018-02-17T21:45:35Z","org":{"id":19627246,"login":"QuantStack","gravatar_id":"","url":"https://api.github.com/orgs/QuantStack","avatar_url":"https://avatars.githubusercontent.com/u/19627246?"}},{"id":"7258421377","type":"WatchEvent","actor":{"id":306951,"login":"nadesai","display_login":"nadesai","gravatar_id":"","url":"https://api.github.com/users/nadesai","avatar_url":"https://avatars.githubusercontent.com/u/306951?"},"repo":{"id":14980049,"name":"sile/hash_ring","url":"https://api.github.com/repos/sile/hash_ring"},"payload":{"action":"started"},"public":true,"created_at":"2018-02-17T07:09:12Z"},{"id":"7258419292","type":"WatchEvent","actor":{"id":306951,"login":"nadesai","display_login":"nadesai","gravatar_id":"","url":"https://api.github.com/users/nadesai","avatar_url":"https://avatars.githubusercontent.com/u/306951?"},"repo":{"id":70087765,"name":"sile/libflate","url":"https://api.github.com/repos/sile/libflate"},"payload":{"action":"started"},"public":true,"created_at":"2018-02-17T07:07:20Z"},{"id":"7248089595","type":"WatchEvent","actor":{"id":306951,"login":"nadesai","display_login":"nadesai","gravatar_id":"","url":"https://api.github.com/users/nadesai","avatar_url":"https://avatars.githubusercontent.com/u/306951?"},"repo":{"id":39698188,"name":"pyparallel/pyparallel","url":"https://api.github.com/repos/pyparallel/pyparallel"},"payload":{"action":"started"},"public":true,"created_at":"2018-02-15T07:45:39Z","org":{"id":8021232,"login":"pyparallel","gravatar_id":"","url":"https://api.github.com/orgs/pyparallel","avatar_url":"https://avatars.githubusercontent.com/u/8021232?"}},{"id":"7248086406","type":"WatchEvent","actor":{"id":306951,"login":"nadesai","display_login":"nadesai","gravatar_id":"","url":"https://api.github.com/users/nadesai","avatar_url":"https://avatars.githubusercontent.com/u/306951?"},"repo":{"id":48399452,"name":"tpn/tracer","url":"https://api.github.com/repos/tpn/tracer"},"payload":{"action":"started"},"public":true,"created_at":"2018-02-15T07:44:36Z"},{"id":"7247534619","type":"WatchEvent","actor":{"id":306951,"login":"nadesai","display_login":"nadesai","gravatar_id":"","url":"https://api.github.com/users/nadesai","avatar_url":"https://avatars.githubusercontent.com/u/306951?"},"repo":{"id":2359378,"name":"madler/zlib","url":"https://api.github.com/repos/madler/zlib"},"payload":{"action":"started"},"public":true,"created_at":"2018-02-15T03:40:41Z"},{"id":"7247245675","type":"WatchEvent","actor":{"id":306951,"login":"nadesai","display_login":"nadesai","gravatar_id":"","url":"https://api.github.com/users/nadesai","avatar_url":"https://avatars.githubusercontent.com/u/306951?"},"repo":{"id":55915565,"name":"walshc/nightlights","url":"https://api.github.com/repos/walshc/nightlights"},"payload":{"action":"started"},"public":true,"created_at":"2018-02-15T01:31:04Z"},{"id":"7247175539","type":"WatchEvent","actor":{"id":306951,"login":"nadesai","display_login":"nadesai","gravatar_id":"","url":"https://api.github.com/users/nadesai","avatar_url":"https://avatars.githubusercontent.com/u/306951?"},"repo":{"id":39015863,"name":"wbg-bigdata/nightlights","url":"https://api.github.com/repos/wbg-bigdata/nightlights"},"payload":{"action":"started"},"public":true,"created_at":"2018-02-15T01:04:32Z","org":{"id":13006221,"login":"wbg-bigdata","gravatar_id":"","url":"https://api.github.com/orgs/wbg-bigdata","avatar_url":"https://avatars.githubusercontent.com/u/13006221?"}},{"id":"7247131752","type":"WatchEvent","actor":{"id":306951,"login":"nadesai","display_login":"nadesai","gravatar_id":"","url":"https://api.github.com/users/nadesai","avatar_url":"https://avatars.githubusercontent.com/u/306951?"},"repo":{"id":39015987,"name":"wbg-bigdata/nightlights-api","url":"https://api.github.com/repos/wbg-bigdata/nightlights-api"},"payload":{"action":"started"},"public":true,"created_at":"2018-02-15T00:48:51Z","org":{"id":13006221,"login":"wbg-bigdata","gravatar_id":"","url":"https://api.github.com/orgs/wbg-bigdata","avatar_url":"https://avatars.githubusercontent.com/u/13006221?"}},{"id":"7247013615","type":"WatchEvent","actor":{"id":306951,"login":"nadesai","display_login":"nadesai","gravatar_id":"","url":"https://api.github.com/users/nadesai","avatar_url":"https://avatars.githubusercontent.com/u/306951?"},"repo":{"id":49978193,"name":"valhalla/valhalla","url":"https://api.github.com/repos/valhalla/valhalla"},"payload":{"action":"started"},"public":true,"created_at":"2018-02-15T00:07:57Z","org":{"id":10232331,"login":"valhalla","gravatar_id":"","url":"https://api.github.com/orgs/valhalla","avatar_url":"https://avatars.githubusercontent.com/u/10232331?"}},{"id":"7246997098","type":"WatchEvent","actor":{"id":306951,"login":"nadesai","display_login":"nadesai","gravatar_id":"","url":"https://api.github.com/users/nadesai","avatar_url":"https://avatars.githubusercontent.com/u/306951?"},"repo":{"id":51717338,"name":"trailbehind/DeepOSM","url":"https://api.github.com/repos/trailbehind/DeepOSM"},"payload":{"action":"started"},"public":true,"created_at":"2018-02-15T00:02:35Z","org":{"id":113243,"login":"trailbehind","gravatar_id":"","url":"https://api.github.com/orgs/trailbehind","avatar_url":"https://avatars.githubusercontent.com/u/113243?"}},{"id":"7242321474","type":"WatchEvent","actor":{"id":306951,"login":"nadesai","display_login":"nadesai","gravatar_id":"","url":"https://api.github.com/users/nadesai","avatar_url":"https://avatars.githubusercontent.com/u/306951?"},"repo":{"id":2325298,"name":"torvalds/linux","url":"https://api.github.com/repos/torvalds/linux"},"payload":{"action":"started"},"public":true,"created_at":"2018-02-14T07:02:43Z"},{"id":"7241552359","type":"WatchEvent","actor":{"id":306951,"login":"nadesai","display_login":"nadesai","gravatar_id":"","url":"https://api.github.com/users/nadesai","avatar_url":"https://avatars.githubusercontent.com/u/306951?"},"repo":{"id":80556103,"name":"SpaceNetChallenge/BuildingDetectors","url":"https://api.github.com/repos/SpaceNetChallenge/BuildingDetectors"},"payload":{"action":"started"},"public":true,"created_at":"2018-02-14T01:52:13Z","org":{"id":20795818,"login":"SpaceNetChallenge","gravatar_id":"","url":"https://api.github.com/orgs/SpaceNetChallenge","avatar_url":"https://avatars.githubusercontent.com/u/20795818?"}},{"id":"7241551996","type":"WatchEvent","actor":{"id":306951,"login":"nadesai","display_login":"nadesai","gravatar_id":"","url":"https://api.github.com/users/nadesai","avatar_url":"https://avatars.githubusercontent.com/u/306951?"},"repo":{"id":65836510,"name":"SpaceNetChallenge/utilities","url":"https://api.github.com/repos/SpaceNetChallenge/utilities"},"payload":{"action":"started"},"public":true,"created_at":"2018-02-14T01:52:05Z","org":{"id":20795818,"login":"SpaceNetChallenge","gravatar_id":"","url":"https://api.github.com/orgs/SpaceNetChallenge","avatar_url":"https://avatars.githubusercontent.com/u/20795818?"}},{"id":"7241548290","type":"WatchEvent","actor":{"id":306951,"login":"nadesai","display_login":"nadesai","gravatar_id":"","url":"https://api.github.com/users/nadesai","avatar_url":"https://avatars.githubusercontent.com/u/306951?"},"repo":{"id":98457039,"name":"SpaceNetChallenge/BuildingDetectors_Round2","url":"https://api.github.com/repos/SpaceNetChallenge/BuildingDetectors_Round2"},"payload":{"action":"started"},"public":true,"created_at":"2018-02-14T01:50:47Z","org":{"id":20795818,"login":"SpaceNetChallenge","gravatar_id":"","url":"https://api.github.com/orgs/SpaceNetChallenge","avatar_url":"https://avatars.githubusercontent.com/u/20795818?"}},{"id":"7241543739","type":"WatchEvent","actor":{"id":306951,"login":"nadesai","display_login":"nadesai","gravatar_id":"","url":"https://api.github.com/users/nadesai","avatar_url":"https://avatars.githubusercontent.com/u/306951?"},"repo":{"id":51117837,"name":"tensorflow/models","url":"https://api.github.com/repos/tensorflow/models"},"payload":{"action":"started"},"public":true,"created_at":"2018-02-14T01:49:24Z","org":{"id":15658638,"login":"tensorflow","gravatar_id":"","url":"https://api.github.com/orgs/tensorflow","avatar_url":"https://avatars.githubusercontent.com/u/15658638?"}},{"id":"7230599657","type":"PushEvent","actor":{"id":306951,"login":"nadesai","display_login":"nadesai","gravatar_id":"","url":"https://api.github.com/users/nadesai","avatar_url":"https://avatars.githubusercontent.com/u/306951?"},"repo":{"id":117744908,"name":"parimarjan/ChasingBalls","url":"https://api.github.com/repos/parimarjan/ChasingBalls"},"payload":{"push_id":2323533795,"size":1,"distinct_size":1,"ref":"refs/heads/master","head":"340d68e7a650ac96097ef2750de865aea6f4da45","before":"33ed124c31c0e45883da07a75a5a30457ee6f42e","commits":[{"sha":"340d68e7a650ac96097ef2750de865aea6f4da45","author":{"email":"nikhilarundesai@gmail.com","name":"Nikhil Desai"},"message":"add requirements","distinct":true,"url":"https://api.github.com/repos/parimarjan/ChasingBalls/commits/340d68e7a650ac96097ef2750de865aea6f4da45"}]},"public":true,"created_at":"2018-02-12T06:21:44Z"},{"id":"7229895547","type":"WatchEvent","actor":{"id":306951,"login":"nadesai","display_login":"nadesai","gravatar_id":"","url":"https://api.github.com/users/nadesai","avatar_url":"https://avatars.githubusercontent.com/u/306951?"},"repo":{"id":78266307,"name":"pcwalton/pathfinder","url":"https://api.github.com/repos/pcwalton/pathfinder"},"payload":{"action":"started"},"public":true,"created_at":"2018-02-12T00:09:13Z"},{"id":"7229892173","type":"WatchEvent","actor":{"id":306951,"login":"nadesai","display_login":"nadesai","gravatar_id":"","url":"https://api.github.com/users/nadesai","avatar_url":"https://avatars.githubusercontent.com/u/306951?"},"repo":{"id":42894575,"name":"servo/webrender","url":"https://api.github.com/repos/servo/webrender"},"payload":{"action":"started"},"public":true,"created_at":"2018-02-12T00:07:07Z","org":{"id":2566135,"login":"servo","gravatar_id":"","url":"https://api.github.com/orgs/servo","avatar_url":"https://avatars.githubusercontent.com/u/2566135?"}},{"id":"7229864614","type":"WatchEvent","actor":{"id":306951,"login":"nadesai","display_login":"nadesai","gravatar_id":"","url":"https://api.github.com/users/nadesai","avatar_url":"https://avatars.githubusercontent.com/u/306951?"},"repo":{"id":11317014,"name":"servo/rust-cssparser","url":"https://api.github.com/repos/servo/rust-cssparser"},"payload":{"action":"started"},"public":true,"created_at":"2018-02-11T23:50:28Z","org":{"id":2566135,"login":"servo","gravatar_id":"","url":"https://api.github.com/orgs/servo","avatar_url":"https://avatars.githubusercontent.com/u/2566135?"}}]